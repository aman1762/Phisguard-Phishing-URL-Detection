# -*- coding: utf-8 -*-
"""finalnotebook6b19394d89

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EZCo_jBAueTeaBtSXqMXtV7oPVbnXGD
"""


# NOTEBOOK.
import kagglehub
taruntiwarihp_phishing_site_urls_path = kagglehub.dataset_download('taruntiwarihp/phishing-site-urls')

print('Data source import complete.')

# üìò 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import joblib  # For saving model

# üìÇ 2. Load Dataset
df = pd.read_csv("phishing_site_urls.csv")  # Replace with your actual file
df.head()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

# üîç 3. Explore the Dataset
print("Dataset shape:", df.shape)
print("Null values:\n", df.isnull().sum())
print("Label distribution:\n", df['Label'].value_counts())

# Pie chart
df['Label'].value_counts().plot.pie(autopct="%1.1f%%", labels=["Good (Legit)", "Bad (Phishing)"], colors=["green", "red"])
plt.title("Label Distribution")
plt.show()

# üßπ 4. Preprocessing
# Encode labels: Good ‚Üí 1 (Legit), Bad ‚Üí 0 (Phishing)
df['Label'] = df['Label'].map({'good': 1, 'bad': 0})
print("\nEncoded Labels:\n", df['Label'].value_counts())

df

# ‚úÇÔ∏è Updated Preprocessing with Stemming
import re
from nltk.stem import SnowballStemmer

# Initialize the stemmer (English language)
stemmer = SnowballStemmer("english")

def clean_url(url):
    # 1. Basic Cleaning (Same as before)
    url = url.lower()
    url = re.sub(r'https?:\/\/', '', url)
    url = re.sub(r'www\d*\.', '', url)
    url = re.sub(r'[^a-z0-9\-\.\/]', ' ', url)
    url = re.sub(r'\s+', ' ', url)

    # 2. Stemming (The New Part!)
    # Split the sentence into words, chop them, and join them back
    words = url.split()
    stemmed_words = [stemmer.stem(word) for word in words]

    return " ".join(stemmed_words)

# Apply this new function to your data
print("Applying Stemming... this might take a minute...")
df['clean_url'] = df['URL'].apply(clean_url)
print("Done! Here is the new cleaned data:")
print(df[['URL', 'clean_url']].head())

# ‚òÅÔ∏è Add this to a new cell to generate Word Clouds
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 1. Separate the Good and Bad sites
# Remember: 0 = Phishing (Bad), 1 = Legit (Good)
phishing_text = " ".join(df[df['Label'] == 0]['clean_url'])
legit_text = " ".join(df[df['Label'] == 1]['clean_url'])

# 2. Create the Word Cloud for Phishing Sites
wordcloud_phishing = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(phishing_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_phishing, interpolation='bilinear')
plt.axis('off')
plt.title("üö® Most Common Words in PHISHING URLs", fontsize=15)
plt.show()

# 3. Create the Word Cloud for Legitimate Sites
wordcloud_legit = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(legit_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_legit, interpolation='bilinear')
plt.axis('off')
plt.title("‚úÖ Most Common Words in LEGITIMATE URLs", fontsize=15)
plt.show()

# üõ†Ô∏è Advanced Feature Engineering (REPLACES Step 5)
from urllib.parse import urlparse
import ipaddress
import pandas as pd
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer # Added this to prevent errors

# 1. Define functions to extract "Structural" features
def get_url_length(url):
    return len(url)

def count_dots(url):
    return url.count('.')

def has_ip_address(url):
    try:
        ipaddress.ip_address(url)
        return 1
    except:
        return 0

def count_slashes(url):
    return url.count('/')

# 2. Apply these functions to create new numerical columns
print("Extracting manual features...")
df['len_url'] = df['URL'].apply(get_url_length)
df['count_dots'] = df['URL'].apply(count_dots)
# Note: has_ip_address usually works better on the raw URL, not cleaned
df['has_ip'] = df['URL'].apply(has_ip_address)
df['count_slashes'] = df['URL'].apply(count_slashes)

# 3. Create the TF-IDF Features (The "Content" part)
print("Vectorizing text...")
vectorizer = TfidfVectorizer(max_features=5000)
X_text = vectorizer.fit_transform(df['clean_url'])

# 4. Create the Manual Features Matrix (The "Structural" part)
X_manual = df[['len_url', 'count_dots', 'has_ip', 'count_slashes']].values

# 5. Combine them!
X_combined = hstack([X_text, X_manual])

print(f"Old Shape (Text Only): {X_text.shape}")
print(f"New Shape (Text + Manual): {X_combined.shape}")
print("‚úÖ Features combined successfully!")

# 6. Prepare for Split
X = X_combined
y = df['Label']

# üîÄ 6. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier

!pip install catboost # Keeping this here just in case of environment resets.
from catboost import CatBoostClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=150, max_depth=25),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "Multinomial NB": MultinomialNB(),
}

results = []
for name, model in models.items():
    print(f"üîç Training: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results.append((name, acc))
    print(f"‚úÖ Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=["Phishing", "Legit"]))
    print("-" * 60)

results_df = pd.DataFrame(results, columns=["Model", "Accuracy"]).sort_values(by="Accuracy", ascending=False)
results_df

# üß† 7. Train Logistic Regression Model
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# üìä 8. Evaluate Model
y_pred = lr_model.predict(X_test)

print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("\nüìÑ Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Purples',
            xticklabels=['Phishing', 'Legit'], yticklabels=['Phishing', 'Legit'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Logistic Regression Confusion Matrix")
plt.show()

sample_urls = [
    "http://login-verification-update.com",
    "https://www.paypal.com",
    "http://free-prize.ru",
    "https://www.apple.com",
    "http://password-reset-verify.org",
    "https://google.com",
    "http://bank-security-alert.co",
    "https://netflix-login-verification.com",
    "http://update-account-info.ru",
    "https://github.com"
]

# üßπ Clean URLs
cleaned_urls = [clean_url(url) for url in sample_urls]

# üî° TF-IDF Vectorize
sample_features = vectorizer.transform(cleaned_urls)

# Predict classes (0 = phishing, 1 = legit)

# 1. Extract manual features from sample_urls
sample_len_url = [get_url_length(url) for url in sample_urls]
sample_count_dots = [count_dots(url) for url in sample_urls]
sample_has_ip = [has_ip_address(url) for url in sample_urls]
sample_count_slashes = [count_slashes(url) for url in sample_urls]

# 2. Create a NumPy array for manual features
X_manual_sample = np.array([sample_len_url, sample_count_dots, sample_has_ip, sample_count_slashes]).T

# 3. Combine TF-IDF features with manual features
# sample_features is already a sparse matrix from vectorizer.transform
# X_manual_sample is a dense numpy array
sample_features_combined = hstack([sample_features, X_manual_sample])

predictions = lr_model.predict(sample_features_combined)

# Predict probabilities (optional)
probabilities = lr_model.predict_proba(sample_features_combined)[:, 1]  # Confidence for class 1 (Legit)

for url, pred, prob in zip(sample_urls, predictions, probabilities):
    label = "‚úÖ Legit" if pred == 1 else "‚ö†Ô∏è Phishing"
    print(f"{url} => {label} ({prob * 100:.2f}%)")

# üíæ 10. Save Model & Vectorizer
joblib.dump(lr_model, "logistic_regression_phishing_model.pkl")
joblib.dump(vectorizer, "tfidf_vectorizer_lr.pkl")
print("Model and vectorizer saved.")

import gradio as gr
import joblib
import re
import ipaddress
import numpy as np
from scipy.sparse import hstack

# 1. Load the saved model and vectorizer
try:
    model = joblib.load("logistic_regression_phishing_model.pkl")
    vectorizer = joblib.load("tfidf_vectorizer_lr.pkl")
    print("‚úÖ Model and Vectorizer loaded successfully!")
except FileNotFoundError:
    print("‚ùå Error: .pkl files not found. Make sure you ran the Training step!")

# 2. Define ALL Helper Functions (Must match training code)
def clean_url(url):
    url = url.lower()
    url = re.sub(r'https?:\/\/', '', url)
    url = re.sub(r'www\d*\.', '', url)
    url = re.sub(r'[^a-z0-9\-\.\/]', ' ', url)
    url = re.sub(r'\s+', ' ', url)
    return url.strip()

def get_url_length(url):
    return len(url)

def count_dots(url):
    return url.count('.')

def has_ip_address(url):
    try:
        ipaddress.ip_address(url)
        return 1
    except:
        return 0

def count_slashes(url):
    return url.count('/')

# 3. Define the Prediction Function
def predict_phishing(url):
    # --- Part A: Text Features ---
    cleaned_url = clean_url(url)
    text_features = vectorizer.transform([cleaned_url]) # Shape: (1, 5000)

    # --- Part B: Manual Features ---
    # We must extract the exact same features we trained on
    f_length = get_url_length(url)
    f_dots = count_dots(url)
    f_ip = has_ip_address(url)
    f_slashes = count_slashes(url)

    # Create a numpy array for these 4 numbers (Shape: 1, 4)
    manual_features = np.array([[f_length, f_dots, f_ip, f_slashes]])

    # --- Part C: Combine & Predict ---
    # Stack them side-by-side to get Shape: (1, 5004)
    combined_features = hstack([text_features, manual_features])

    probabilities = model.predict_proba(combined_features)[0]

    return {
        "‚ö†Ô∏è Phishing": float(probabilities[0]),
        "‚úÖ Legitimate": float(probabilities[1])
    }

# 4. Create Interface
interface = gr.Interface(
    fn=predict_phishing,
    inputs=gr.Textbox(label="Enter URL", placeholder="Type or paste a URL here..."),
    outputs=gr.Label(num_top_classes=2, label="Result"),
    title="üõ°Ô∏è Advanced Phishing Detector",
    description="This hybrid model analyzes both the **text content** and the **structure** (dots, length, etc.) of the URL.",
    theme="default"
)

if __name__ == "__main__":
    interface.launch(share=True)
